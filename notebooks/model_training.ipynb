{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Model Training \n",
    "Run the cells to reproduce the results\n",
    "\n",
    "### Data and Models Used in Hisia\n",
    "\n",
    "**Data:** 2016 TrustPilot's 254,464 Danish reviews' body and stars and [8 fake reviews]*20 see notes for the explanation.<br>\n",
    "&ensp; _Update_: 2021-10-02: Political Data from [Sentiment Analysis on Comments from Danish Political Articles on Social Media](https://github.com/steffan267/Sentiment-Analysis-on-Danish-Social-Media)\n",
    "\n",
    "**Models**<br>\n",
    "Hisia, _LogisticRegression_ with SAGA, a variant of Stochastic Average Gradient (SAG), as a solver. L2 penalty was select for the base model. Test score **accuracy is ca. 93%** and **recall of 93%**. SAGA is a faster solver for large datasets (both rows and columns wise). As a stochastic gradient, the memory of the previous gradient is incorporated/feed-forward to achieve faster convergence rate. Seeds of 42 was set in data split, and 42 in a model for reproducibility.\n",
    "\n",
    "HisiaTrain, _SGDClassifier_, Stochastic Gradient Descent learner with smooth loss 'modified_huber as loss function and L2 penalty. Test score **accurance  94%** and **recall of 94%**. SGDClassifier was select because of partial_fit. It allows batch/online training.\n",
    "\n",
    "**Note:** This score reflects models in regards to TrustPilot reviews style of writing.<b>\n",
    " >8*10 fake reviews. TrustPilot reviews are directed toward products and services. Words like 'elsker'(love) or 'hader'(hate) are rarely used. To make sure the model learns such a relationship, I added 8 reviews and duplicated them 10 times. These new sentences did not increase or decrease the model accurance but added the correct coefficient of love, hate and (ikke dårligt) not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Author Prayson W. Daniel\n",
      "\n",
      "Last updated: 2022-02-09T17:32:33.827667+01:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.12\n",
      "IPython version      : 8.0.1\n",
      "\n",
      "pandas      : 1.4.0\n",
      "numpy       : 1.22.2\n",
      "matplotlib  : 3.5.1\n",
      "scikit-learn: 1.0.2\n",
      "lemmy       : 2.1.0\n",
      "dill        : 0.3.4\n",
      "\n",
      "Compiler    : GCC 9.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.10.16.3-microsoft-standard-WSL2\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 12\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -uniz --author \"Author Prayson W. Daniel\" -vm -p pandas,numpy,matplotlib,scikit-learn,lemmy,dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import joblib\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import dill\n",
    "import lemmy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV, SGDClassifier\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import show_diagram\n",
    "from helpers import show_most_informative_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (15,5)\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This stops were custom made from both unknown Danish stops words and products and services related words such as delivery, package, post office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = '../data'\n",
    "PATH_TO_STOPWORDS = '../hisia/models/data'\n",
    "STOP_WORDS = joblib.load(f'{PATH_TO_STOPWORDS}/stops.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer separates emojis from the words, removes digits, repetive words and stop words and lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = lemmy.load('da')\n",
    "\n",
    "\n",
    "# Add more stopwords\n",
    "STOP_WORDS.update({\"kilometer\", \"alme\", \"bank\", \"brand\", \"dansk\", \"presi\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"kilometer\" in STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(blob, stop_words=STOP_WORDS, remove_digits=True):\n",
    "    \n",
    "    if stop_words is None:\n",
    "        stop_words = {}\n",
    "    \n",
    "    blob = blob.lower()\n",
    "    \n",
    "     # eyes [nose] mouth | mouth [nose] eyes pattern\n",
    "    emoticons = r\"(?:[<>]?[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8][<>]?)\"\n",
    "    emoticon_re = re.compile(emoticons, re.VERBOSE | re.I | re.UNICODE)\n",
    "    \n",
    "    text = re.sub(r'[\\W]+', ' ', blob)\n",
    "    \n",
    "    # remove 3+ repetitive characters i.e. hellllo -> hello, jaaaa -> jaa \n",
    "    repetitions = re.compile(r'(.)\\1{2,}')\n",
    "    text = repetitions.sub(r'\\1\\1', text)\n",
    "    \n",
    "    # remove 2+ repetitive words e.g. hej hej hej -> hej\n",
    "    \n",
    "    repetitions = re.compile(r'\\b(\\w+)\\s+(\\1\\s*)+\\b')\n",
    "    text = repetitions.sub(r'\\1 ', text)\n",
    "    \n",
    "    \n",
    "    # 14år --> 14 år\n",
    "    text = re.sub(r'([0-9]+(\\.[0-9]+)?)', r' \\1 ', text).strip()\n",
    "    \n",
    "    emoji = ''.join(re.findall(emoticon_re, blob))\n",
    "    \n",
    "       \n",
    "    # remove stopwords\n",
    "    text_nostop = [word for word in text.split() if word not in stop_words]\n",
    "    \n",
    "    # tokenization lemmatize\n",
    "    lemmatized_text = [lemmatizer.lemmatize('', word)[-1]  \n",
    "                                 for word in text_nostop]\n",
    "    \n",
    "    remove_stopwords = ' '.join(word for word in lemmatized_text if len(word)>1)\n",
    "    \n",
    "    if remove_digits:\n",
    "        remove_stopwords = re.sub(r'\\b\\d+\\b', '', remove_stopwords)\n",
    "    \n",
    "\n",
    "    # remove extra spaces\n",
    "    remove_stopwords = ' '.join(remove_stopwords .split())\n",
    "    result = f'{remove_stopwords} {emoji}'.encode('utf-8').decode('utf-8')\n",
    "       \n",
    "    \n",
    "    return result.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vred', 'ikke', ':(']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('Jeg er vred på, at jeg ikke fik min pakke :( kilometer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f'{PATH_TO_DATA}/data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake reviews to teach our model the missing relationsh that is not found in TP reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.DataFrame([('men elsker elsker', 1,5), \n",
    "                   ('elsker det ikke', 0, 1), \n",
    "                   ('ikke dårligt', 1, 5),\n",
    "                   ('elsker skat, kæreste, tilbedte, dyrebare, elskling, darling, hjerte, hjertenskær; ven; veninde', 1, 5),\n",
    "                   ('dårlig: syg, sløj, utilpas, ilde tilpas, upasselig, snavs, indisponeret;'\n",
    "                    'sygelig, usund, ond, slet; arg, uheldig, umulig, elendig, under al kritik,'\n",
    "                    'dødssyg, skidt, skral, krank, ussel, ikke noget at samle på, talentløs, uantagelig,'\n",
    "                    'uacceptabel, forkastelig; ikke noget at råbe hurra for, ikke noget at skrive hjem om,'\n",
    "                    'noget skidt (lort, pis), andenklasses, tredjeklasses (osv.), ringe, halvgod, ikke nogen'\n",
    "                    'ørn til, ikke ens stærke side, ens svage punkt, som en brækket arm; sjusket; ufordelagtig,'\n",
    "                    'ufyldestgørende, utilstrækkelig, utilfredsstillende, middelmådig, under lavmålet, uduelig,'\n",
    "                    'udygtig, uhensigtsmæssig, forkert, tarvelig; skadelig, ødelæggende, fordærvet, ubrugelig;'\n",
    "                    'ubehagelig, væmmelig, ulystbetonet; dys-; utiltalende, usympatisk, kedelig', 0, 1),\n",
    "                    ('20.000 kroner. Det er, hvad man som arbejdstager burde få ekstra i lønningsposen,'\n",
    "                    ' hvis man skal kunne acceptere at have en dårlig chef.', 0,1),\n",
    "                   ('kærlighed, hvordan elsker vi hinanden godt – uanset hvem vi elsker?',1,5),\n",
    "                   ('jeg hader dig', 0, 1),\n",
    "                   \n",
    "                  ]*20, \n",
    "                  columns='features target stars'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New data from [\"Sentiment Analysis on Comments from Danish Political Articles on Social Media\"](https://github.com/lucaspuvis/SAM/blob/master/Thesis.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = \"https://raw.githubusercontent.com/steffan267/Sentiment-Analysis-on-Danish-Social-Media/master/all_sentences.csv\"\n",
    "ds = pd.read_csv(SAM, names=[\"target\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_json(\"../data/steffan267_SAM.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ds\n",
    "      .loc[lambda d: d['target'].ne(0), [\"target\", ]]\n",
    "      .assign(target= lambda d: np.where(d[\"target\"].gt(0), 1, 0))\n",
    "      .value_counts()\n",
    "      .rename(index ={0: \"negative\", 1:\"positive\"})\n",
    "      .to_frame(name=\"observations\")    \n",
    ")\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    ds\n",
    "      .loc[lambda d: d[\"target\"].ne(0), [\"features\", \"target\"]]\n",
    "      .assign(target= lambda d: np.where(d[\"target\"].gt(0), 1, 0))\n",
    "       \n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.concat([dt, ds], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt.to_json('../data/data_custom.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['features'], \n",
    "                                                     df['target'],\n",
    "                                                     test_size=.2,\n",
    "                                                     random_state=42,\n",
    "                                                     stratify=df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = (pd.concat([X_train, dt['features']] ,ignore_index=True),\n",
    "                    pd.concat([y_train, dt['target']] ,ignore_index=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Traing Size: {X_train.shape[0]}\\nTest Size: {X_test.shape[0]:>8}')\n",
    "print(f'\\nTraing Size\\n\\tPositive||Negative Samples\\n\\t  {y_train[y_train==1].shape[0]}||{y_train[y_train==0].shape[0]}')\n",
    "print(f'\\nTest Size\\n\\tPositive||Negative Samples\\n\\t  {y_test[y_test==1].shape[0]}||{y_test[y_test==0].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia = Pipeline(steps =[\n",
    "        ('count_verctorizer',  CountVectorizer(ngram_range=(1, 2), \n",
    "                                 max_features=150000,\n",
    "                                 tokenizer=tokenizer, \n",
    "                                 stop_words=STOP_WORDS\n",
    "                                )\n",
    "        ),\n",
    "        ('feature_selector', SelectKBest(chi2, k=10000)),\n",
    "        ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "        ('logistic_regression', LogisticRegressionCV(cv=5,\n",
    "                                                    solver='saga',\n",
    "                                                    scoring='accuracy',\n",
    "                                                    max_iter=200,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    random_state=42,\n",
    "                                                    verbose=0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hisia.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diagram(hisia, X_train, y_train, X_test, y_test, compare_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = hisia.named_steps['count_verctorizer'].get_feature_names_out()\n",
    "best_features = [feature_names[i] for i in hisia.named_steps['feature_selector'].get_support(indices=True)]\n",
    "predictor =  hisia.named_steps['logistic_regression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "print(f'Showing {N} models learned features for negative and postive decisions')\n",
    "print('_'*70)\n",
    "print('\\n')\n",
    "show_most_informative_features(best_features, predictor, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [negative, positive] probability\n",
    "hisia.predict_proba(['det er ikke okay!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.predict_proba(['det er ikke dårligt!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hisia.predict_proba(['jeg kan lide det!']), \n",
    " hisia.predict_proba(['jeg kan ikke lide det!']),\n",
    " hisia.predict_proba(['jeg elsker det!']),\n",
    " hisia.predict_proba(['jeg elsker det slet ikke!'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_max = ['Jeg er vred på, at jeg ikke fik min pakke :( elsker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.named_steps['logistic_regression'].random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.predict_proba(['']) # model is positive :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.predict(mad_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = hisia.predict_proba(mad_max)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.decision_function(mad_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = hisia.named_steps['count_verctorizer'].transform(mad_max)\n",
    "v = hisia.named_steps['feature_selector'].transform(v)\n",
    "v = pd.DataFrame.sparse.from_spmatrix(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up = {index:(token,coef) for index, coef, token in \n",
    "           zip(range(len(best_features)),\n",
    "               hisia.named_steps['logistic_regression'].coef_[0], \n",
    "               best_features)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "{look_up[item] for item in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.named_steps['logistic_regression'].intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = [look_up[item] for item in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝑓(𝐱): 𝑝(𝐱) = 1 / (1 + exp(−𝑓(𝐱))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(1 + np.exp(-(g[0][1] + g[1][1] + hisia.named_steps['logistic_regression'].intercept_[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia.decision_function(mad_max)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = np.where(df[0] > .5, 'negative', 'positive')\n",
    "\n",
    "df.columns = ['negative_probability','positive_probability','sentiment']\n",
    "\n",
    "Sentiment = namedtuple('Sentiment', ['sentiment','positive_probability', 'negative_probability'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Sentiment(**df.round(3).to_dict(orient='index')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrainable Model SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hisia_trainer =Pipeline(steps =[\n",
    "                ('count_verctorizer',  CountVectorizer(ngram_range=(1, 2), \n",
    "                                         max_features=100000,\n",
    "                                         tokenizer=tokenizer, \n",
    "                                        )\n",
    "                ),\n",
    "                ('feature_selector', SelectKBest(chi2, k=5000)),\n",
    "                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "                ('modified_hubern', SGDClassifier(loss='modified_huber', \n",
    "                                                      random_state=7,\n",
    "                                                      max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hisia_trainer.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for partil_fit we have to split the pipeline to transformation and scoring\n",
    "hisia_trainer.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diagram(hisia_trainer, X_train, y_train, X_test, y_test, compare_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = hisia_trainer.named_steps['count_verctorizer'].get_feature_names()\n",
    "best_features = [feature_names[i] for i in hisia_trainer.named_steps['feature_selector'].get_support(indices=True)]\n",
    "predictor =  hisia_trainer.named_steps['modified_hubern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "print(f'Showing {N} models learned features for negative and postive decisions')\n",
    "print('_'*70)\n",
    "print('\\n')\n",
    "show_most_informative_features(best_features, predictor, n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hisia",
   "language": "python",
   "name": "hisia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
